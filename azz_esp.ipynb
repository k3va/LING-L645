{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/k3va/LING-L645/blob/main/azz_esp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install GPUtil\n",
        "\n",
        "import torch\n",
        "from GPUtil import showUtilization as gpu_usage\n",
        "from numba import cuda\n",
        "\n",
        "def free_gpu_cache():\n",
        "    print(\"Initial GPU Usage\")\n",
        "    gpu_usage()                             \n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    cuda.select_device(0)\n",
        "    cuda.close()\n",
        "    cuda.select_device(0)\n",
        "\n",
        "    print(\"GPU Usage after emptying the cache\")\n",
        "    gpu_usage()\n",
        "\n",
        "free_gpu_cache() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wCOtogl9Jnz",
        "outputId": "297c4975-31a0-434a-fe78-7df61079aed6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: GPUtil in /usr/local/lib/python3.8/dist-packages (1.4.0)\n",
            "Initial GPU Usage\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  0% |  1% |\n",
            "GPU Usage after emptying the cache\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  2% |  1% |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check GPU run or not\n",
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "!nvidia-smi\n",
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU8gyt3afLNe",
        "outputId": "2476bcf5-388d-4707-efba-0eeaf2bde1cf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tue Dec 13 00:49:02 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P0    28W /  70W |    102MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Feb_14_21:12:58_PST_2021\n",
            "Cuda compilation tools, release 11.2, V11.2.152\n",
            "Build cuda_11.2.r11.2/compiler.29618528_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beagAGw5t5bK"
      },
      "source": [
        "%%capture\n",
        "# Local installation\n",
        "!git clone https://github.com/speechbrain/speechbrain/\n",
        "%cd /content/speechbrain/\n",
        "!pip install -r requirements.txt\n",
        "!pip install -e ."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2xRRTu12y1w"
      },
      "source": [
        "%%capture\n",
        "# For pip installation\n",
        "!pip install speechbrain"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kZj4OG0t_Hn3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92b5c544-798d-464b-f889-505c0910d1b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "\n",
        "#random sample with dataset OR after master JSON data creation\n",
        "import random\n",
        "sample = random.random()\n",
        "if sample <0.8:\n",
        "    <file is train set>\n",
        "elif 0.8 < sample  < 0.9:\n",
        "    <file is  dev set>\n",
        "else:\n",
        "<file is test set>\n",
        "traindict, devdict, testdict = {}, {}, {}\n",
        "\n",
        "#iterate and add to appropriate dict\n",
        "\"\"\"\n",
        "import glob\n",
        "\n",
        "path = \"/content/drive/MyDrive/Research\"\n",
        "files = glob.glob(path+\"/sample/*.wav\")\n",
        "print(files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93_D6X-tEyLH",
        "outputId": "09626109-6ba0-4f9c-c44c-301e6ea70d9a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import random\n",
        "import logging\n",
        "from speechbrain.utils.data_utils import get_all_files, download_file\n",
        "from speechbrain.dataio.dataio import read_audio\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "SAMPLERATE = 16000\n",
        "\n",
        "def prepare_mini_librispeech(\n",
        "    data_folder,\n",
        "    save_json_train,\n",
        "    save_json_valid,\n",
        "    save_json_test,\n",
        "    split_ratio=[80, 10, 10],\n",
        "):\n",
        "\n",
        "    # List files and create manifest from list\n",
        "    logger.info(\n",
        "        f\"Creating {save_json_train}, {save_json_valid}, and {save_json_test}\"\n",
        "    )\n",
        "    extension = [\".wav\"]\n",
        "    wav_list = get_all_files(data_folder, match_and=extension)\n",
        "\n",
        "    # Random split the signal list into train, valid, and test sets.\n",
        "    data_split = split_sets(wav_list, split_ratio)\n",
        "\n",
        "    # Creating json files\n",
        "    create_json(data_split[\"train\"], save_json_train)\n",
        "    create_json(data_split[\"valid\"], save_json_valid)\n",
        "    create_json(data_split[\"test\"], save_json_test)\n",
        "\n",
        "\n",
        "def create_json(wav_list, json_file):\n",
        "    \"\"\"\n",
        "    Creates the json file given a list of wav files.\n",
        "    Arguments\n",
        "    ---------\n",
        "    wav_list : list of str\n",
        "        The list of wav files.\n",
        "    json_file : str\n",
        "        The path of the output json file\n",
        "    \"\"\"\n",
        "    # Processing all the wav files in the list\n",
        "    json_dict = {}\n",
        "    for wav_file in wav_list:\n",
        "\n",
        "        # Reading the signal (to retrieve duration in seconds)\n",
        "        signal = read_audio(wav_file)\n",
        "        duration = signal.shape[0] / SAMPLERATE\n",
        "\n",
        "        # Manipulate path to get relative path and uttid\n",
        "        path_parts = wav_file.split(os.path.sep)\n",
        "        uttid, _ = os.path.splitext(path_parts[-1])             #uttid=name of file, _= file extension\n",
        "        relative_path = os.path.join(\"{data_root}\", *path_parts[-5:])\n",
        "\n",
        "        # Getting speaker-id from utterance-id\n",
        "        spk_id = uttid.split(\"_\")[0]\n",
        "\n",
        "        # Create entry for this utterance\n",
        "        json_dict[uttid] = {\n",
        "            \"wav\": relative_path,\n",
        "            \"length\": duration,\n",
        "            \"spk_id\": spk_id,\n",
        "        }\n",
        "\n",
        "    # Writing the dictionary to the json file\n",
        "    with open(json_file, mode=\"w\") as json_f:\n",
        "        json.dump(json_dict, json_f, indent=2)\n",
        "\n",
        "    logger.info(f\"{json_file} successfully created!\")\n",
        "\n",
        "\n",
        "def split_sets(wav_list, split_ratio):\n",
        "    \"\"\"Randomly splits the wav list into training, validation, and test lists.\n",
        "    Note that a better approach is to make sure that all the classes have the\n",
        "    same proportion of samples (e.g, spk01 should have 80% of samples in\n",
        "    training, 10% validation, 10% test, the same for speaker2 etc.). This\n",
        "    is the approach followed in some recipes such as the Voxceleb one. For\n",
        "    simplicity, we here simply split the full list without necessarily respecting\n",
        "    the split ratio within each class.\n",
        "    Arguments\n",
        "    ---------\n",
        "    wav_lst : list\n",
        "        list of all the signals in the dataset\n",
        "    split_ratio: list\n",
        "        List composed of three integers that sets split ratios for train, valid,\n",
        "        and test sets, respectively. For instance split_ratio=[80, 10, 10] will\n",
        "        assign 80% of the sentences to training, 10% for validation, and 10%\n",
        "        for test.\n",
        "    Returns\n",
        "    ------\n",
        "    dictionary containing train, valid, and test splits.\n",
        "    \"\"\"\n",
        "    # Random shuffle of the list\n",
        "    random.shuffle(wav_list)\n",
        "    tot_split = sum(split_ratio)\n",
        "    tot_snts = len(wav_list)\n",
        "    data_split = {}\n",
        "    splits = [\"train\", \"valid\"]\n",
        "\n",
        "    for i, split in enumerate(splits):\n",
        "        n_snts = int(tot_snts * split_ratio[i] / tot_split)\n",
        "        data_split[split] = wav_list[0:n_snts]\n",
        "        del wav_list[0:n_snts]\n",
        "    data_split[\"test\"] = wav_list\n",
        "\n",
        "    return data_split\n",
        "\n",
        "#prepare_mini_librispeech(path+'/sample', path+'/save_json_train.json', path+'/save_json_valid.json', path+'/save_json_test.json')"
      ],
      "metadata": {
        "id": "3RWf64MPHTqi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtMw7x0ybFlI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97016752-254d-411b-9c24-f0608ce476b6"
      },
      "source": [
        "#Training the prebuilt speechbrain model on our sample data-manifest files\n",
        "%cd /content/drive/MyDrive/Research/training\n",
        "!python train.py train.yaml --number_of_epochs=2 #--device='cpu'"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Research/training\n",
            "./rirs_noises.zip exists. Skipping download\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: ./results/speaker_id/1986\n",
            "my_tools - Creating train.json, valid.json, and test.json\n",
            "my_tools - train.json successfully created!\n",
            "my_tools - valid.json successfully created!\n",
            "my_tools - test.json successfully created!\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - 4.5M trainable parameters in SpkIdBrain\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            " 50% 1/2 [00:03<00:03,  3.00s/it, train_loss=-.372]\n",
            "speechbrain.core - Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 327, in <module>\n",
            "    spk_id_brain.fit(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/speechbrain/core.py\", line 1153, in fit\n",
            "    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/speechbrain/core.py\", line 1004, in _fit_train\n",
            "    for batch in t:\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tqdm/std.py\", line 1195, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 628, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 671, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 61, in fetch\n",
            "    return self.collate_fn(data)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/speechbrain/dataio/batch.py\", line 125, in __init__\n",
            "    padded = PaddedData(*padding_func(values, **padding_kwargs))\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/speechbrain/utils/data_utils.py\", line 442, in batch_pad_right\n",
            "    padded, valid_percent = pad_right_to(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/speechbrain/utils/data_utils.py\", line 370, in pad_right_to\n",
            "    assert len(target_shape) == tensor.ndim\n",
            "AssertionError\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h3j9_6thH1_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3tnXnrWc2My"
      },
      "source": [
        "As you can see from the prints, both the validation and training **losses are decreasing** very fast in the first epochs. Then, we basically see some minor improvements and performance oscillations.\n",
        "\n",
        "At the end of the training, the **validation error should go to zero** (or very close to zero).\n",
        "\n",
        "The task proposed in this tutorial is very easy because we only have to classify the 28 speakers of the mini-librispeech dataset. Take this tutorial just as an example that explains how to set up all the needed components to develop a speech classifier. [Please, refer to our voxceleb recipes if you would like to see an example on a popular speaker recognition dataset](https://github.com/speechbrain/speechbrain/tree/develop/recipes/VoxCeleb)\n",
        "\n",
        "\n",
        "\n",
        "Before diving into the code, let's see which files/folders are generated in the specified `output_folder`:\n",
        "\n",
        "*   `train_log.txt`: contains the statistics (e.g, train_loss, valid_loss) computed at each epoch.\n",
        "*   `log.txt`: is a more detailed logger containing the timestamps for each basic operation.\n",
        "*  `env.log`: shows all the dependencies used with their corresponding version (useful for replicability).\n",
        "\n",
        "*  `train.py`, `hyperparams.yaml`:  are a copy of the experiment file along with the corresponding hyperparameters (for replicability).\n",
        "\n",
        "* `save`:  is the place where we store the learned model.\n",
        "\n",
        "In the `save` folder, you find subfolders containing the checkpoints saved during training (in the format `CKPT+data+time`). Typically, you find here two checkpoints: the **best** (i.e, the oldest one) and the **latest** (i.e, the most recent one). If you find only a single checkpoint it means that the last epoch is also the best.\n",
        "\n",
        "Inside each checkpoint, we store all the information needed to **resume training** (e.g, models, optimizers, schedulers, epoch counter, etc.). The parameters of the embedding models are reported in `embedding_model.ckpt` file,\n",
        "while the ones of the classifier are in `classifier.ckpt`. This is just a binary format readable with `torch.load`.\n",
        "\n",
        "The save folder contains the **label encoder** (`label_encoder.txt`) as well, which maps each speaker-id entry to their corresponding indexes.\n",
        "\n",
        "```\n",
        "'163' => 0\n",
        "'7312' => 1\n",
        "'7859' => 2\n",
        "'19' => 3\n",
        "'1737' => 4\n",
        "'6272' => 5\n",
        "'1970' => 6\n",
        "'2416' => 7\n",
        "'118' => 8\n",
        "'6848' => 9\n",
        "'4680' => 10\n",
        "'460' => 11\n",
        "'3664' => 12\n",
        "'3242' => 13\n",
        "'1898' => 14\n",
        "'7367' => 15\n",
        "'1088' => 16\n",
        "'3947' => 17\n",
        "'3526' => 18\n",
        "'1867' => 19\n",
        "'8629' => 20\n",
        "'332' => 21\n",
        "'4640' => 22\n",
        "'2136' => 23\n",
        "'669' => 24\n",
        "'5789' => 25\n",
        "'32' => 26\n",
        "'226' => 27\n",
        "================\n",
        "'starting_index' => 0\n",
        "```\n",
        "\n",
        "\n",
        "As usual, we implement the system with an experiment file `train.py` and a hyperparameter file called `train.yaml`.\n",
        "\n",
        "### **Hyperparameters**\n",
        "The yaml file contains all the modules and hyperparameters need to implement the desired classifier.\n",
        "[You can take a look into the full train.yaml file here](https://github.com/speechbrain/speechbrain/blob/develop/templates/speaker_id/train.yaml).\n",
        "\n",
        "In the first part, we specify some basic settings, such as the seed and the path of the output folder:\n",
        "\n",
        "```yaml\n",
        "# Seed needs to be set at top of yaml, before objects with parameters are made\n",
        "seed: 1986\n",
        "__set_seed: !!python/object/apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# If you plan to train a system on an HPC cluster with a big dataset,\n",
        "# we strongly suggest doing the following:\n",
        "# 1- Compress the dataset in a single tar or zip file.\n",
        "# 2- Copy your dataset locally (i.e., the local disk of the computing node).\n",
        "# 3- Uncompress the dataset in the local folder.\n",
        "# 4- Set data_folder with the local path.\n",
        "# Reading data from the local disk of the compute node (e.g. $SLURM_TMPDIR with SLURM-based clusters) is very important.\n",
        "# It allows you to read the data much faster without slowing down the shared filesystem.\n",
        "data_folder: ./data\n",
        "output_folder: !ref ./results/speaker_id/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "```\n",
        "\n",
        "We then specify the path of the data manifest files for training, validation, and test:\n",
        "\n",
        "```yaml\n",
        "# Path where data manifest files will be stored\n",
        "# The data manifest files are created by the data preparation script.\n",
        "train_annotation: train.json\n",
        "valid_annotation: valid.json\n",
        "test_annotation: test.json\n",
        "```\n",
        "\n",
        "These files will be automatically created when calling the data preparation script ([mini_librispeech_prepare.py](https://github.com/speechbrain/speechbrain/blob/develop/templates/speaker_id/mini_librispeech_prepare.py)) from the experiment file (`train.py`).\n",
        "\n",
        "\n",
        "Next, we set up the `train_logger` and declare the `error_stats` objects that will gather statistics on the classification error rate:\n",
        "\n",
        "\n",
        "```yaml\n",
        "# The train logger writes training statistics to a file, as well as stdout.\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "```\n",
        "\n",
        "\n",
        "We can now specify some training hyperparameters such as the number of epochs, the batch size, the learning rate, the number of epochs, and the embedding dimensionality.\n",
        "\n",
        "\n",
        "```yaml\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "# Feature parameters\n",
        "n_mels: 23\n",
        "\n",
        "# Training Parameters\n",
        "sample_rate: 16000\n",
        "number_of_epochs: 35\n",
        "batch_size: 16\n",
        "lr_start: 0.001\n",
        "lr_final: 0.0001\n",
        "n_classes: 28 # In this case, we have 28 speakers\n",
        "emb_dim: 512 # dimensionality of the embeddings\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "```\n",
        "\n",
        "The variable `ckpt_interval_minutes` can be used to save checkpoints every N minutes within a training epoch. In some cases, one epoch might take several hours, and saving the checkpoint periodically is a good and safe practice. This feature is not really needed for this simple tutorial based on a tiny dataset.\n",
        "\n",
        "We can now define the most important modules that are needed to train our model:\n",
        "\n",
        "```yaml\n",
        "# Added noise and reverb come from OpenRIR dataset, automatically\n",
        "# downloaded and prepared with this Environmental Corruption class.\n",
        "env_corrupt: !new:speechbrain.lobes.augment.EnvCorrupt\n",
        "    openrir_folder: !ref <data_folder>\n",
        "    babble_prob: 0.0\n",
        "    reverb_prob: 0.0\n",
        "    noise_prob: 1.0\n",
        "    noise_snr_low: 0\n",
        "    noise_snr_high: 15\n",
        "\n",
        "# Adds speech change + time and frequency dropouts (time-domain implementation)\n",
        "# # A small speed change help to improve the performance of speaker-id as well.\n",
        "augmentation: !new:speechbrain.lobes.augment.TimeDomainSpecAugment\n",
        "    sample_rate: !ref <sample_rate>\n",
        "    speeds: [95, 100, 105]\n",
        "\n",
        "# Feature extraction\n",
        "compute_features: !new:speechbrain.lobes.features.Fbank\n",
        "    n_mels: !ref <n_mels>\n",
        "\n",
        "# Mean and std normalization of the input features\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "# To design a custom model, either just edit the simple CustomModel\n",
        "# class that's listed here, or replace this `!new` call with a line\n",
        "# pointing to a different file you've defined.\n",
        "embedding_model: !new:custom_model.Xvector\n",
        "    in_channels: !ref <n_mels>\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    tdnn_blocks: 5\n",
        "    tdnn_channels: [512, 512, 512, 512, 1500]\n",
        "    tdnn_kernel_sizes: [5, 3, 3, 1, 1]\n",
        "    tdnn_dilations: [1, 2, 3, 1, 1]\n",
        "    lin_neurons: !ref <emb_dim>\n",
        "\n",
        "classifier: !new:custom_model.Classifier\n",
        "    input_shape: [null, null, !ref <emb_dim>]\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    lin_blocks: 1\n",
        "    lin_neurons: !ref <emb_dim>\n",
        "    out_neurons: !ref <n_classes>\n",
        "\n",
        "# The first object passed to the Brain class is this \"Epoch Counter\"\n",
        "# which is saved by the Checkpointer so that training can be resumed\n",
        "# if it gets interrupted at any point.\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "# Objects in \"modules\" dict will have their parameters moved to the correct\n",
        "# device, as well as having train()/eval() called on them by the Brain class.\n",
        "modules:\n",
        "    compute_features: !ref <compute_features>\n",
        "    env_corrupt: !ref <env_corrupt>\n",
        "    augmentation: !ref <augmentation>\n",
        "    embedding_model: !ref <embedding_model>\n",
        "    classifier: !ref <classifier>\n",
        "    mean_var_norm: !ref <mean_var_norm>\n",
        "```\n",
        "The augmentation part is based on both `env_corrupt` (that adds noise and reverberation) and `augmentation`(that adds time/frequency dropouts and speed change).\n",
        "For more information on these modules, please take a look at the tutorials on [enviromental corruption](https://colab.research.google.com/drive/1mAimqZndq0BwQj63VcDTr6_uCMC6i6Un?usp=sharing) and the one on [speech augmentation](https://colab.research.google.com/drive/1JJc4tBhHNXRSDM2xbQ3Z0jdDQUw4S5lr?usp=sharing).\n",
        "\n",
        "\n",
        "\n",
        "We conclude the hyperparameter specification with the declaration of the optimizer, learning rate scheduler, and the checkpointer:\n",
        "\n",
        "\n",
        "```yaml\n",
        "# This optimizer will be constructed by the Brain class after all parameters\n",
        "# are moved to the correct device. Then it will be added to the checkpointer.\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_start>\n",
        "\n",
        "# This function manages learning rate annealing over the epochs.\n",
        "# We here use the simple lr annealing method that linearly decreases\n",
        "# the lr from the initial value to the final one.\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler\n",
        "    initial_value: !ref <lr_start>\n",
        "    final_value: !ref <lr_final>\n",
        "    epoch_count: !ref <number_of_epochs>\n",
        "\n",
        "# This object is used for saving the state of training both so that it\n",
        "# can be resumed if it gets interrupted, and also so that the best checkpoint\n",
        "# can be later loaded for evaluation or inference.\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        embedding_model: !ref <embedding_model>\n",
        "        classifier: !ref <classifier>\n",
        "        normalizer: !ref <mean_var_norm>\n",
        "        counter: !ref <epoch_counter>\n",
        "```\n",
        "\n",
        "In this case, we use Adam as an optimizer and a linear learning rate decay over the 15 epochs.\n",
        "\n",
        "Let's now save the best model into a separate folder (useful for the inference part explained later):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwoN5Vq0dFYe"
      },
      "source": [
        "# Create folder for best model\n",
        "!mkdir /content/best_model/\n",
        "\n",
        "# Copy label encoder\n",
        "!cp results/speaker_id/1986/save/label_encoder.txt /content/best_model/\n",
        "\n",
        "# Copy best model\n",
        "!cp \"`ls -td results/speaker_id/1986/save/CKPT* | tail -1`\"/* /content/best_model/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnCM5xuy85P4"
      },
      "source": [
        "### **Experiment file**\n",
        "Let's now take a look into how the objects, functions, and hyperparameters declared in the yaml file are used in `train.py` to implement the classifier.\n",
        "\n",
        "Let's start from the main of the `train.py`:\n",
        "\n",
        "\n",
        "```python\n",
        "# Recipe begins!\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Reading command line arguments.\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training).\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides.\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Data preparation, to be run on only one process.\n",
        "    sb.utils.distributed.run_on_main(\n",
        "        prepare_mini_librispeech,\n",
        "        kwargs={\n",
        "            \"data_folder\": hparams[\"data_folder\"],\n",
        "            \"save_json_train\": hparams[\"train_annotation\"],\n",
        "            \"save_json_valid\": hparams[\"valid_annotation\"],\n",
        "            \"save_json_test\": hparams[\"test_annotation\"],\n",
        "            \"split_ratio\": [80, 10, 10],\n",
        "        },\n",
        "    )\n",
        "```\n",
        "\n",
        "We here do some preliminary operations such as parsing the command line, initializing the distributed data-parallel (needed if multiple GPUs are used), creating the output folder, and reading the yaml file.\n",
        "\n",
        "After reading the yaml file with `load_hyperpyyaml`, all the objects declared in the hyperparameter files are initialized and available in a dictionary form (along with the other functions and parameters reported in the yaml file).\n",
        "For instance,  we will have `hparams['embedding_model']`, `hparams['classifier']`, `hparams['batch_size']`, etc.\n",
        "\n",
        "We also run the data preparation script `prepare_mini_librispeech` that creates the data manifest files. It is wrapped with `sb.utils.distributed.run_on_main` because this operation writes the manifest files on disk and this must be done on a single process even in a multi-GPU DDP scenario. For more information on how to use multiple GPUs, [please take a look into this tutorial](https://colab.research.google.com/drive/13pBUacPiotw1IvyffvGZ-HrtBr9T6l15?usp=sharing).\n",
        "\n",
        "\n",
        "#### **Data-IO Pipeline**\n",
        "We then call a special function that creates the dataset objects for training, validation, and test.\n",
        "\n",
        "```python\n",
        "    # Create dataset objects \"train\", \"valid\", and \"test\".\n",
        "    datasets = dataio_prep(hparams)\n",
        "```\n",
        "\n",
        "Let's take a closer look into that.\n",
        "\n",
        "\n",
        "```python\n",
        "def dataio_prep(hparams):\n",
        "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
        "    It also defines the data processing pipeline through user-defined functions.\n",
        "    We expect `prepare_mini_librispeech` to have been called before this,\n",
        "    so that the `train.json`, `valid.json`,  and `valid.json` manifest files\n",
        "    are available.\n",
        "    Arguments\n",
        "    ---------\n",
        "    hparams : dict\n",
        "        This dictionary is loaded from the `train.yaml` file, and it includes\n",
        "        all the hyperparameters needed for dataset construction and loading.\n",
        "    Returns\n",
        "    -------\n",
        "    datasets : dict\n",
        "        Contains two keys, \"train\" and \"valid\" that correspond\n",
        "        to the appropriate DynamicItemDataset object.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialization of the label encoder. The label encoder assignes to each\n",
        "    # of the observed label a unique index (e.g, 'spk01': 0, 'spk02': 1, ..)\n",
        "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
        "\n",
        "    # Define audio pipeline\n",
        "    @sb.utils.data_pipeline.takes(\"wav\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav):\n",
        "        \"\"\"Load the signal, and pass it and its length to the corruption class.\n",
        "        This is done on the CPU in the `collate_fn`.\"\"\"\n",
        "        sig = sb.dataio.dataio.read_audio(wav)\n",
        "        return sig\n",
        "\n",
        "    # Define label pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"spk_id\")\n",
        "    @sb.utils.data_pipeline.provides(\"spk_id\", \"spk_id_encoded\")\n",
        "    def label_pipeline(spk_id):\n",
        "        yield spk_id\n",
        "        spk_id_encoded = label_encoder.encode_label_torch(spk_id)\n",
        "        yield spk_id_encoded\n",
        "\n",
        "    # Define datasets. We also connect the dataset with the data processing\n",
        "    # functions defined above.\n",
        "    datasets = {}\n",
        "    hparams[\"dataloader_options\"][\"shuffle\"] = False\n",
        "    for dataset in [\"train\", \"valid\", \"test\"]:\n",
        "        datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
        "            json_path=hparams[f\"{dataset}_annotation\"],\n",
        "            replacements={\"data_root\": hparams[\"data_folder\"]},\n",
        "            dynamic_items=[audio_pipeline, label_pipeline],\n",
        "            output_keys=[\"id\", \"sig\", \"spk_id_encoded\"],\n",
        "        )\n",
        "\n",
        "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
        "    # Please, take a look into the lab_enc_file to see the label to index\n",
        "    # mappinng.\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file,\n",
        "        from_didatasets=[datasets[\"train\"]],\n",
        "        output_key=\"spk_id\",\n",
        "    )\n",
        "\n",
        "    return datasets\n",
        "```\n",
        "\n",
        "The first part is just a declaration of the `CategoricalEncoder` that will be used to convert categorical labels into their corresponding indexes.\n",
        "\n",
        "\n",
        "You can then notice that we expose the audio and label processing functions.\n",
        "\n",
        "The `audio_pipeline` takes the path of the audio signal (`wav`) and reads it. It returns a tensor containing the read speech sentence. The entry in input to this function (i.e, `wav`) must have the same name of the corresponding key in the data manifest file:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"163-122947-0045\": {\n",
        "    \"wav\": \"{data_root}/LibriSpeech/train-clean-5/163/122947/163-122947-0045.flac\",\n",
        "    \"length\": 14.335,\n",
        "    \"spk_id\": \"163\"\n",
        "  },\n",
        "}\n",
        "```\n",
        "\n",
        "Similarly, we define another function called `label_pipeline` for processing the utterance-level labels and put them in a format usable by the defined model. The function reads the string `spk_id` defined in the JSON file and encodes it with the categorical encoder.\n",
        "\n",
        "We then create the `DynamicItemDataset` and connect it with the processing functions defined above. We define the **desired output keys** to expose. These keys will be available in the brain class within the batch variable as:\n",
        "- batch.id\n",
        "- batch.sig\n",
        "- batch.spk_id_encoded\n",
        "\n",
        "The last part of the function is dedicated to the initialization of the label encoder. The label encoder takes in input the training dataset and assigns a different index to all the `spk_id` entries founded. These indexes will correspond to the output indexes of the classifier.\n",
        "\n",
        "[For more information on the data loader, please take a look into this tutorial](https://colab.research.google.com/drive/1AiVJZhZKwEI4nFGANKXEe-ffZFfvXKwH?usp=sharing)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "After the definition of the datasets, the main function can go ahead with the  initialization and use of the brain class:\n",
        "\n",
        "```python\n",
        "    # Initialize the Brain object to prepare for mask training.\n",
        "    spk_id_brain = SpkIdBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # The `fit()` method iterates the training loop, calling the methods\n",
        "    # necessary to update the parameters of the model. Since all objects\n",
        "    # with changing state are managed by the Checkpointer, training can be\n",
        "    # stopped at any point, and will be resumed on next call.\n",
        "    spk_id_brain.fit(\n",
        "        epoch_counter=spk_id_brain.hparams.epoch_counter,\n",
        "        train_set=datasets[\"train\"],\n",
        "        valid_set=datasets[\"valid\"],\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = spk_id_brain.evaluate(\n",
        "        test_set=datasets[\"test\"],\n",
        "        min_key=\"error\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "```\n",
        "The `fit` method performs training, while the test is performed with the `evaluate` one. The training and validation data loaders are given in input to the fit method, while the test dataset is fed into the evaluate method.\n",
        "\n",
        "Let's now take a look into the most important methods defined in the brain class.\n",
        "\n",
        "\n",
        "\n",
        "#### **Forward Computations**\n",
        "\n",
        "Let's start with the `forward` function, which defines all the computations needed to transform the input audio into the output predictions.\n",
        "\n",
        "\n",
        "```python\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Runs all the computation of that transforms the input into the\n",
        "        output probabilities over the N classes.\n",
        "        Arguments\n",
        "        ---------\n",
        "        batch : PaddedBatch\n",
        "            This batch object contains all the relevant tensors for computation.\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
        "        Returns\n",
        "        -------\n",
        "        predictions : Tensor\n",
        "            Tensor that contains the posterior probabilities over the N classes.\n",
        "        \"\"\"\n",
        "\n",
        "        # We first move the batch to the appropriate device.\n",
        "        batch = batch.to(self.device)\n",
        "\n",
        "        # Compute features, embeddings, and predictions\n",
        "        feats, lens = self.prepare_features(batch.sig, stage)\n",
        "        embeddings = self.modules.embedding_model(feats, lens)\n",
        "        predictions = self.modules.classifier(embeddings)\n",
        "\n",
        "        return predictions\n",
        "```\n",
        "\n",
        "In this case, the chain of computation is very simple. We just put the batch on the right device and compute the acoustic features. We then process the features with the TDNN encoder that outputs a fixed-size tensor. The latter feeds a classifier that outputs the posterior probabilities over the N classes (in this case the 28 speakers). Data augmentation is added in the prepare_features method:\n",
        "\n",
        "```python\n",
        "    def prepare_features(self, wavs, stage):\n",
        "        \"\"\"Prepare the features for computation, including augmentation.\n",
        "        Arguments\n",
        "        ---------\n",
        "        wavs : tuple\n",
        "            Input signals (tensor) and their relative lengths (tensor).\n",
        "        stage : sb.Stage\n",
        "            The current stage of training.\n",
        "        \"\"\"\n",
        "        wavs, lens = wavs\n",
        "\n",
        "        # Add augmentation if specified. In this version of augmentation, we\n",
        "        # concatenate the original and the augment batches in a single bigger\n",
        "        # batch. This is more memory-demanding, but helps to improve the\n",
        "        # performance. Change it if you run OOM.\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            if hasattr(self.modules, \"env_corrupt\"):\n",
        "                wavs_noise = self.modules.env_corrupt(wavs, lens)\n",
        "                wavs = torch.cat([wavs, wavs_noise], dim=0)\n",
        "                lens = torch.cat([lens, lens])\n",
        "\n",
        "            if hasattr(self.hparams, \"augmentation\"):\n",
        "                wavs = self.hparams.augmentation(wavs, lens)\n",
        "\n",
        "        # Feature extraction and normalization\n",
        "        feats = self.modules.compute_features(wavs)\n",
        "        feats = self.modules.mean_var_norm(feats, lens)\n",
        "\n",
        "        return feats, lens\n",
        "```\n",
        "In particular, when the environmental corruption is declared in the yaml file, we concatenate in the same batch both the clean and the augmented version of the signals.\n",
        "\n",
        "This approach doubles the batch size (and this the needed GPU memory), but it implements a very **powerful regularizer**.  Having both the clean and the noisy version of the signal within the same batch forces the gradient to point into a direction of the parameter space that is **robust against signal distortions**.\n",
        "\n",
        "#### **Compute Objectives**\n",
        "\n",
        "Let's take a look now into the `compute_objectives` method that takes in input the targets, the predictions, and estimates a loss function:\n",
        "\n",
        "```python\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss given the predicted and targeted outputs.\n",
        "        Arguments\n",
        "        ---------\n",
        "        predictions : tensor\n",
        "            The output tensor from `compute_forward`.\n",
        "        batch : PaddedBatch\n",
        "            This batch object contains all the relevant tensors for computation.\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
        "        Returns\n",
        "        -------\n",
        "        loss : torch.Tensor\n",
        "            A one-element tensor used for backpropagating the gradient.\n",
        "        \"\"\"\n",
        "\n",
        "        _, lens = batch.sig\n",
        "        spkid, _ = batch.spk_id_encoded\n",
        "\n",
        "        # Concatenate labels (due to data augmentation)\n",
        "        if stage == sb.Stage.TRAIN and hasattr(self.modules, \"env_corrupt\"):\n",
        "            spkid = torch.cat([spkid, spkid], dim=0)\n",
        "            lens = torch.cat([lens, lens])\n",
        "\n",
        "        # Compute the cost function\n",
        "        loss = sb.nnet.losses.nll_loss(predictions, spkid, lens)\n",
        "\n",
        "        # Append this batch of losses to the loss metric for easy\n",
        "        self.loss_metric.append(\n",
        "            batch.id, predictions, spkid, lens, reduction=\"batch\"\n",
        "        )\n",
        "\n",
        "        # Compute classification error at test time\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics.append(batch.id, predictions, spkid, lens)\n",
        "\n",
        "        return loss\n",
        "```\n",
        "The predictions in input are those computed in the forward method. The cost function is evaluated by comparing these predictions with the target labels. This is done with the Negative Log-Likelihood (NLL) loss.\n",
        "\n",
        "####**Other methods**\n",
        "Beyond these two important functions, we have some other methods that are used by the brain class. The `on_state_starts` gets called at the beginning of each epoch and it is used to set up statistics trackers. The `on_stage_end` one is called at the end of each stage (e.g, at the end of each training epoch) and mainly takes care of statistic management, learning rate annealing, and checkpointing. [For a more detailed description of the brain class, please take a look into this tutorial](https://colab.research.google.com/drive/12bg3aUdr9mTfOGqcB5pSMABoIKPgiwcM?usp=sharing). For more information on checkpointing, [take a look here](https://colab.research.google.com/drive/1VH7U0oP3CZsUNtChJT2ewbV_q1QX8xre?usp=sharing)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LnRq1_cpPXZ"
      },
      "source": [
        "## **Step 3: Inference**\n",
        "\n",
        "At this point, we can use the trained classifier to perform **predictions on new data**.  Speechbrain made available some classes ([take a look here](https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/pretrained/interfaces.py)) such as the `EncoderClassifier` one that can make inference easier. The class can also be used to extract some embeddings at the output of the encoder.\n",
        "\n",
        "Let's see first how can we used it to load our best xvector model (trained on Voxceleb and stored on HuggingFace) to compute some embeddings and perform a speaker classification:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvvY0dCbx5Sv"
      },
      "source": [
        "import torchaudio\n",
        "from speechbrain.pretrained import EncoderClassifier\n",
        "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-xvect-voxceleb\")\n",
        "signal, fs =torchaudio.load('/content/speechbrain/samples/audio_samples/example1.wav')\n",
        "\n",
        "# Compute speaker embeddings\n",
        "embeddings = classifier.encode_batch(signal)\n",
        "\n",
        "# Perform classification\n",
        "output_probs, score, index, text_lab = classifier.classify_batch(signal)\n",
        "\n",
        "# Posterior log probabilities\n",
        "print(output_probs)\n",
        "\n",
        "# Score (i.e, max log posteriors)\n",
        "print(score)\n",
        "\n",
        "# Index of the predicted speaker\n",
        "print(index)\n",
        "\n",
        "# Text label of the predicted speaker\n",
        "print(text_lab)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAZci6oSzdh_"
      },
      "source": [
        "For those of you interested in speaker verification, we also created an inference interface called `SpeakerRecognition`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-enSWy_z8CF"
      },
      "source": [
        "from speechbrain.pretrained import SpeakerRecognition\n",
        "verification = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"pretrained_models/spkrec-ecapa-voxceleb\")\n",
        "\n",
        "file1 = '/content/speechbrain/samples/audio_samples/example1.wav'\n",
        "file2 = '/content/speechbrain/samples/audio_samples/example2.flac'\n",
        "\n",
        "score, prediction = verification.verify_files(file1, file2)\n",
        "\n",
        "print(score)\n",
        "print(prediction) # True = same speaker, False=Different speakers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbS7ZncE3IfM"
      },
      "source": [
        "But, *how does this work with our custom classifier that we trained before*?\n",
        "\n",
        "At this point, some options are available to you. For a full overview of all of them, [please take a look into this tutorial](https://colab.research.google.com/drive/1aFgzrUv3udM_gNJNUoLaHIm78QHtxdIz?usp=sharing).\n",
        "\n",
        "We here only show how you can use the existing `EncoderClassifier` on the model that we just trained.\n",
        "\n",
        "\n",
        "### **Use the EncoderClassifier interface on your model**\n",
        "\n",
        "The [EncoderClassidier class](https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/pretrained/interfaces.py#L591) takes a pre-trained model and performs inference on it with the following methods:\n",
        "\n",
        "- **encode_batch**: applies the encoder to an input batch and returns some encoded embeddings.\n",
        "- **classify_batch**: performs a full classification step and returns the output probabilities of the classifier, the best score, the index of the best class, and its label in text format (see example above).\n",
        "\n",
        "To use this interface with the model trained before, we have to create an **inference yaml** file which is a bit different from that use for training. The main differences are the following:\n",
        "\n",
        "1. You can remove all the hyperparameters and objects needed for training only. You can just keep the part related to the model definition.\n",
        "2. You have to allocate a `Categorical encoder` object that allows you to transform indexes into text labels.\n",
        "3. You have to use the pre-trainer to link your model with their corresponding files.\n",
        "\n",
        "The inference yaml file looks like that:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ys41HanSaCys"
      },
      "source": [
        "%%writefile /content/best_model/hparams_inference.yaml\n",
        "\n",
        "# #################################\n",
        "# Basic inference parameters for speaker-id. We have first a network that\n",
        "# computes some embeddings. On the top of that, we employ a classifier.\n",
        "#\n",
        "# Author:\n",
        "#  * Mirco Ravanelli 2021\n",
        "# #################################\n",
        "\n",
        "# pretrain folders:\n",
        "pretrained_path: /content/best_model/\n",
        "\n",
        "\n",
        "# Model parameters\n",
        "n_mels: 23\n",
        "sample_rate: 16000\n",
        "n_classes: 28 # In this case, we have 28 speakers\n",
        "emb_dim: 512 # dimensionality of the embeddings\n",
        "\n",
        "# Feature extraction\n",
        "compute_features: !new:speechbrain.lobes.features.Fbank\n",
        "    n_mels: !ref <n_mels>\n",
        "\n",
        "# Mean and std normalization of the input features\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "# To design a custom model, either just edit the simple CustomModel\n",
        "# class that's listed here, or replace this `!new` call with a line\n",
        "# pointing to a different file you've defined.\n",
        "embedding_model: !new:custom_model.Xvector\n",
        "    in_channels: !ref <n_mels>\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    tdnn_blocks: 5\n",
        "    tdnn_channels: [512, 512, 512, 512, 1500]\n",
        "    tdnn_kernel_sizes: [5, 3, 3, 1, 1]\n",
        "    tdnn_dilations: [1, 2, 3, 1, 1]\n",
        "    lin_neurons: !ref <emb_dim>\n",
        "\n",
        "classifier: !new:custom_model.Classifier\n",
        "    input_shape: [null, null, !ref <emb_dim>]\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    lin_blocks: 1\n",
        "    lin_neurons: !ref <emb_dim>\n",
        "    out_neurons: !ref <n_classes>\n",
        "\n",
        "label_encoder: !new:speechbrain.dataio.encoder.CategoricalEncoder\n",
        "\n",
        "# Objects in \"modules\" dict will have their parameters moved to the correct\n",
        "# device, as well as having train()/eval() called on them by the Brain class.\n",
        "modules:\n",
        "    compute_features: !ref <compute_features>\n",
        "    embedding_model: !ref <embedding_model>\n",
        "    classifier: !ref <classifier>\n",
        "    mean_var_norm: !ref <mean_var_norm>\n",
        "\n",
        "pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer\n",
        "    loadables:\n",
        "        embedding_model: !ref <embedding_model>\n",
        "        classifier: !ref <classifier>\n",
        "        label_encoder: !ref <label_encoder>\n",
        "    paths:\n",
        "        embedding_model: !ref <pretrained_path>/embedding_model.ckpt\n",
        "        classifier: !ref <pretrained_path>/classifier.ckpt\n",
        "        label_encoder: !ref <pretrained_path>/label_encoder.txt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDsp3atIiKSq"
      },
      "source": [
        "As you can see, we only have the model definition here (not optimizers, checkpoiter, etc). The last part of the yaml file manages pretraining, where we bind model objects with their pre-training files created at training time.\n",
        "\n",
        "Let's now perform inference with the `EncoderClassifier` class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fT8ON1iiuQY"
      },
      "source": [
        "from speechbrain.pretrained import EncoderClassifier\n",
        "\n",
        "classifier = EncoderClassifier.from_hparams(source=\"/content/best_model/\", hparams_file='hparams_inference.yaml', savedir=\"/content/best_model/\")\n",
        "\n",
        "# Perform classification\n",
        "audio_file = 'data/LibriSpeech/train-clean-5/5789/70653/5789-70653-0036.flac'\n",
        "signal, fs = torchaudio.load(audio_file) # test_speaker: 5789\n",
        "output_probs, score, index, text_lab = classifier.classify_batch(signal)\n",
        "print('Target: 5789, Predicted: ' + text_lab[0])\n",
        "\n",
        "# Another speaker\n",
        "audio_file = 'data/LibriSpeech/train-clean-5/460/172359/460-172359-0012.flac'\n",
        "signal, fs =torchaudio.load(audio_file) # test_speaker: 460\n",
        "output_probs, score, index, text_lab = classifier.classify_batch(signal)\n",
        "print('Target: 460, Predicted: ' + text_lab[0])\n",
        "\n",
        "# And if you want to extract embeddings...\n",
        "embeddings = classifier.encode_batch(signal)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSxRDjEnB0FA"
      },
      "source": [
        "The `EncoderClassifier` interface assumes that your model has the following modules specified in the yaml file:\n",
        "\n",
        "- *compute_features*: that manages feature extraction from the raw audio signal\n",
        "- *mean_var_norm*: that performs feature normalization\n",
        "- *embedding_model*: that converts features into fix-size embeddings.\n",
        "- *classifier*: that performs a final classification over N classes on the top o the embeddings.\n",
        "\n",
        "If your model cannot be structured in this way, you can always customize the `EncoderClassifier` interface to fulfill your needs.\n",
        "[Please, take a look into this tutorial for more information](https://colab.research.google.com/drive/1aFgzrUv3udM_gNJNUoLaHIm78QHtxdIz?usp=sharing).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3pu0M42Pqju"
      },
      "source": [
        "## **Extension to different tasks**\n",
        "In a general case, you might have your own data and classification task and you would like to use your own model. Let's comment a bit more on how you can customize your recipe.\n",
        "\n",
        "**Suggestion**:  start from a recipe that is working (like the one used for this template) and only do the minimal modifications needed to customize it. Test your model step by step. Make sure your model can overfit on a tiny dataset composed of few sentences. If it doesn't overfit there is likely a bug in your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tImuOg5XP3CY"
      },
      "source": [
        "### **Train with your data on your task**\n",
        "What about if I have to solve another utterance-level classification task such as **language-id**, **emotion recognition**, **sound classification**, **keyword spotting** on my data?\n",
        "\n",
        "All you have to do is:\n",
        "1. Change the JSON with the annotations needed for your task.\n",
        "2. Change the data pipeline in `train.py` to be compliant with the new annotations.\n",
        "\n",
        "#### **Change the JSON**\n",
        "This tutorial expects  JSON files like this:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"163-122947-0045\": {\n",
        "    \"wav\": \"{data_root}/LibriSpeech/train-clean-5/163/122947/163-122947-0045.flac\",\n",
        "    \"length\": 14.335,\n",
        "    \"spk_id\": \"163\"\n",
        "  },\n",
        "  \"7312-92432-0025\": {\n",
        "    \"wav\": \"{data_root}/LibriSpeech/train-clean-5/7312/92432/7312-92432-0025.flac\",\n",
        "    \"length\": 12.01,\n",
        "    \"spk_id\": \"7312\"\n",
        "  },\n",
        "  \"7859-102519-0036\": {\n",
        "    \"wav\": \"{data_root}/LibriSpeech/train-clean-5/7859/102519/7859-102519-0036.flac\",\n",
        "    \"length\": 11.965,\n",
        "    \"spk_id\": \"7859\"\n",
        "  },\n",
        "}\n",
        "```\n",
        "\n",
        "However, you can add here all the entries that you want. For instance, if you would like to solve a language-id task, the JSON file should look like this:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"sentence001\": {\n",
        "    \"wav\": \"{data_root}/your_path/your_file1.wav\",\n",
        "    \"length\": 10.335,\n",
        "    \"lang_id\": \"Italian\"\n",
        "  },\n",
        "{\n",
        "  \"sentence002\": {\n",
        "    \"wav\": \"{data_root}/your_path/your_file2.wav\",\n",
        "    \"length\": 12.335,\n",
        "    \"lang_id\": \"French\"\n",
        "  },\n",
        "}\n",
        "```\n",
        "\n",
        "If you would like to solve an emotion recognition task, it will look like that:\n",
        "\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"sentence001\": {\n",
        "    \"wav\": \"{data_root}/your_path/your_file1.wav\",\n",
        "    \"length\": 10.335,\n",
        "    \"emotion\": \"Happy\"\n",
        "  },\n",
        "{\n",
        "  \"sentence002\": {\n",
        "    \"wav\": \"{data_root}/your_path/your_file2.wav\",\n",
        "    \"length\": 12.335,\n",
        "    \"emotion\": \"Sad\"\n",
        "  },\n",
        "}\n",
        "```\n",
        "To create the data manifest files, you have to **parse your dataset and create JSON files** with a unique ID for each sentence, the path of the audio signal (wav), the length of the speech sentence in seconds (length), and the annotations that you want.\n",
        "\n",
        "#### **Change train.py**\n",
        "The only thing to remember is that the name entries in the JSON file must match with what the dataloader expectes in `train.py`.  For instance, if you defined an emotion key in JSON, you should have it in the dataio pipeline of `train.py` something like this:\n",
        "\n",
        "```python\n",
        "    # Define label pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"emotion\")\n",
        "    @sb.utils.data_pipeline.provides(\"emotion\", \"emotion_encoded\")\n",
        "    def label_pipeline(emotion):\n",
        "        yield emotion\n",
        "        emotion_encoded = label_encoder.encode_label_torch(emotion)\n",
        "        yield emotion_encoded\n",
        "```\n",
        "\n",
        "Basically, you have to replace the `spk_id` entry with the `emotion` one everywhere in the code. That's all!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVCCe6cXPzJ0"
      },
      "source": [
        "### **Train with your own model**\n",
        "At some point, you might have your own model and you would like to plug it into the speech recognition pipeline.\n",
        "For instance, you might wanna replace our xvector encoder with something different.\n",
        "\n",
        "To do that, you have to create your own class and specify there the list of computations for your neural network. You can take a look into the models already existing in [speechbrain.lobes.models](https://github.com/speechbrain/speechbrain/tree/develop/speechbrain/lobes/models). If your model is a plain pipeline of computations, you can use the [sequential container](https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/lobes/models/CRDNN.py#L14). If the model is a more complex chain of computations, you can create it as an instance of `torch.nn.Module` and define there the `__init__` and `forward` methods like [here](https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/lobes/models/Xvector.py#L18).\n",
        "\n",
        "Once you defined your model, you only have to declare it in the yaml file and use it in `train.py`\n",
        "\n",
        "**Important:**\n",
        "When plugging a new model, you have to tune again the most important hyperparameters of the system (e.g, learning rate, batch size, and the architectural parameters) to make it working well.\n",
        "\n",
        "\n",
        "#### **ECAPA-TDNN model**\n",
        "One model that we find particularly effective for speaker recognition is the ECAPA-TDNN one [implemented here](https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/lobes/models/ECAPA_TDNN.py).\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1iRwioXBSYKC2TnNg7ZFAXSydBQghxKBN)\n",
        "\n",
        "As shown in Figure, the ECAPA-TDNN architecture is based on the popular x-vector topology and it introduces **several enhancements** to create more robust speaker embeddings.\n",
        "\n",
        "The pooling layer uses a **channel- and context-dependent attention** mechanism, which allows the network to attend different frames per channel.\n",
        "1-dimensional **SqueezeExcitation** (SE)  blocks rescale the channels of the intermediate frame-level feature maps to insert **global context** information in the locally operating convolutional blocks.\n",
        "Next, the integration of 1-dimensional **Res2-blocks** improves performance while simultaneously reducing the total parameter count\n",
        "by using grouped convolutions in a hierarchical way.\n",
        "\n",
        "Finally, **Multi-layer Feature Aggregation (MFA)** merges complementary information before the statistics pooling by concatenating the final frame-level feature map with an intermediate feature\n",
        "maps of preceding layers.\n",
        "\n",
        "The network is trained by optimizing the **AAMsoftmax** loss on the speaker identities in the training corpus. The AAM-softmax is a powerful enhancement compared to the regular softmax loss in the context of fine-grained classification and verification problems. It directly optimizes the\n",
        "cosine distance between the speaker embeddings.\n",
        "\n",
        "\n",
        "The model turned out to work amazingly well for [speaker verification](https://arxiv.org/abs/2005.07143) and [speaker diarization](https://arxiv.org/abs/2104.01466). We found it very effective in other utterance-level classification tasks such as language-id, emotion recognition, and keyword spotting.\n",
        "\n",
        "[Please take a look into the original paper for more info](https://arxiv.org/abs/2005.07143)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4pPJ0k3lJZj"
      },
      "source": [
        "\n",
        "\n",
        "## **Conclusion**\n",
        "\n",
        "In this tutorial, we showed how to create an utterance-level classifier from scratch using SpeechBrain. The proposed system contains all the basic ingredients to develop a state-of-the-art system (i.e., data augmentation, feature extraction, encoding, statistical pooling, classifier, etc)\n",
        "\n",
        "We described all the steps using a small dataset only. In a real case you have to train with much more data (see for instance our [Voxceleb recipe](https://github.com/speechbrain/speechbrain/tree/develop/recipes/VoxCeleb))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-Trg_abjUTd"
      },
      "source": [
        "## Related Tutorials\n",
        "1. [YAML hyperpatameter specification](https://colab.research.google.com/drive/1Pg9by4b6-8QD2iC0U7Ic3Vxq4GEwEdDz?usp=sharing)\n",
        "2. [Brain Class](https://colab.research.google.com/drive/1fdqTk4CTXNcrcSVFvaOKzRfLmj4fJfwa?usp=sharing)\n",
        "3. [Checkpointing](https://colab.research.google.com/drive/1VH7U0oP3CZsUNtChJT2ewbV_q1QX8xre?usp=sharing)\n",
        "4. [Data-io](https://colab.research.google.com/drive/1AiVJZhZKwEI4nFGANKXEe-ffZFfvXKwH?usp=sharing)\n",
        "5. [ASR from Scratch](https://colab.research.google.com/drive/1aFgzrUv3udM_gNJNUoLaHIm78QHtxdIz?usp=sharing)\n",
        "6. [Speech Features](https://colab.research.google.com/drive/1CI72Xyay80mmmagfLaIIeRoDgswWHT_g?usp=sharing)\n",
        "7. [Speech Augmentation](https://colab.research.google.com/drive/1JJc4tBhHNXRSDM2xbQ3Z0jdDQUw4S5lr?usp=sharing)\n",
        "8. [Environmental Corruption](https://colab.research.google.com/drive/1mAimqZndq0BwQj63VcDTr6_uCMC6i6Un?usp=sharing)\n",
        "9. [MultiGPU Training](https://colab.research.google.com/drive/13pBUacPiotw1IvyffvGZ-HrtBr9T6l15?usp=sharing)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfznz3wlEnn8"
      },
      "source": [
        "# **About SpeechBrain**\n",
        "- Website: https://speechbrain.github.io/\n",
        "- Code: https://github.com/speechbrain/speechbrain/\n",
        "- HuggingFace: https://huggingface.co/speechbrain/\n",
        "\n",
        "\n",
        "# **Citing SpeechBrain**\n",
        "Please, cite SpeechBrain if you use it for your research or business.\n",
        "\n",
        "```bibtex\n",
        "@misc{speechbrain,\n",
        "  title={{SpeechBrain}: A General-Purpose Speech Toolkit},\n",
        "  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and François Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\n",
        "  year={2021},\n",
        "  eprint={2106.04624},\n",
        "  archivePrefix={arXiv},\n",
        "  primaryClass={eess.AS},\n",
        "  note={arXiv:2106.04624}\n",
        "}\n",
        "```"
      ]
    }
  ]
}